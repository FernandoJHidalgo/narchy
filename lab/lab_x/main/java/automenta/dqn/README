https://groups.google.com/forum/#!topic/deeplearning4j/aKFwOGSRf98

Hi all,

Newbie here. I am investigating the practicality of implementing a Deep Q-learning Network using the NN lib provided by Deeplearning4j.
The network architecture is pretty simple, 2 hidden fully connected rectifier layers, 1 regression output layer.
Training method is simply SGD (for each sample, we pass in a target dimension and its target value)

Which part of the DL4J api should I look into first?
Thanks very much in advance!


Adam Gibson
Apr 23


The MultiLayerNetwork. Take a look at the 0.0.3.3 examples (https://github.com/deeplearning4j/dl4j-0.0.3.3-examples)
Build from master for now on this. This will give you the composable networks.
- show quoted text -


http://arxiv.org/pdf/1507.04296v2.pdf
Algorithm 1 Distributed DQN Algorithm
Initialise replay memory D to size P.
Initialise the training network for the action-value function
Q(s, a; Î¸) with weights Î¸ and target network
Q(s, a; Î¸
âˆ’) with weights Î¸
âˆ’ = Î¸.
for episode = 1 to M do
Initialise the start state to s1.
Update Î¸ from parameters Î¸
+ of the parameter server.
for t = 1 to T do
With probability  take a random action at or else
at = argmax
a
Q(s, a; Î¸).
Execute the action in the environment and observe
the reward rt and the next state st+1. Store
(st, at, rt, st+1) in D.
Update Î¸ from parameters Î¸
+ of the parameter
server.
Sample random mini-batch from D. And for each
tuple (si
, ai
, ri
, si+1) set target yt as
if si+1 is terminal then
yt = ri
else
yt = ri + Î³max
a0
Q(si+1, a0
; Î¸
âˆ’)
end if
Calculate the loss Lt = (yt âˆ’ Q(si
, ai
; Î¸)
2
).
Compute gradients with respect to the network parameters
Î¸ using equation 2.
Send gradients to the parameter server.
Every global N steps sync Î¸
âˆ’ with parameters Î¸
+
from the parameter server.
end for
end for